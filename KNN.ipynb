import pandas as pd
df = pd.read_csv('/Users/junnu/Downloads/Data Science /Assignment Questions/KNN/Zoo.csv')
df
df.dtypes

import matplotlib.pyplot as plt
df.hist(figsize=(10, 8))
plt.show()

import seaborn as sns
df.boxplot(figsize=(10, 8))
plt.show()

import matplotlib.pyplot as plt
(df.groupby('predator')['hair'].value_counts(normalize=True) * 100).unstack().plot(kind='pie', subplots=True)
_ = plt.title('Proportion of Animals with Hair by Predator')

pd.crosstab(df['type'], df['domestic']).plot(kind="bar", figsize=(8, 6), title="Class wise Domestic & Non-Domestic Count");
plt.plot();
pd.crosstab(df['type'], df['aquatic']).plot(kind="bar", figsize=(10, 8));

df.isnull().sum()
df['animal name'].value_counts()
duplicates = df['animal name'].value_counts()
duplicates[duplicates > 1]
frog = df[df['animal name'] == 'frog']
frog
df.drop(26)

numerical_columns = df.select_dtypes(include=["int64", "float64"]).columns
for col in numerical_columns:
  Q1 = df[col].quantile(0.25)
  Q3 = df[col].quantile(0.75)
  IQR = Q3 - Q1
  lower_bound = Q1 - 1.5 * IQR
  upper_bound = Q3 + 1.5 * IQR
  df[col] = df[col].clip(lower=lower_bound, upper=upper_bound)
df.describe()

from sklearn.preprocessing import StandardScaler
SS = StandardScaler() 
SS_X  = SS.fit_transform(df.iloc[:,1:16])
SS_X
X = SS_X
Y = df.iloc[:,16]

from sklearn.model_selection import train_test_split
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier(n_neighbors=5)
knn.fit(X_train, Y_train)
Y_train_pred = knn.predict(X_train)
Y_test_pred = knn.predict(X_test)

train_accuracy = knn.score(X_train, Y_train)
test_accuracy = knn.score(X_test, Y_test)

print("Training Accuracy:", train_accuracy.round(3))
print("Test Accuracy:", test_accuracy.round(3))

from sklearn.metrics import log_loss
Training_loss = log_loss(Y_train,Y_train_pred)
Test_loss = log_loss(Y_test,Y_test_pred)

print("Training_loss:", Training_loss.round(3))
print("Test_loss:", Test_loss.round(3))

training_accuracy = []
test_accuracy = []

import numpy as np
knn = KNeighborsClassifier(n_neighbors=5)
from sklearn.metrics import accuracy_score
for i in range(1,200,1):
    X_train,X_test,Y_train,Y_test   = train_test_split(X,Y,test_size=0.20,random_state=i)
    knn.fit(X_train,Y_train)
    y_pred_train = knn.predict(X_train)
    y_pred_test = knn.predict(X_test)
    training_accuracy.append(accuracy_score(Y_train,y_pred_train))
    test_accuracy.append(accuracy_score(Y_test,y_pred_test))

print("Crossvalidation: Training accuracy",np.mean(training_accuracy).round(2))
print("Crossvalidation: Test accuracy",np.mean(test_accuracy).round(2))

import numpy as np
from sklearn.metrics import accuracy_score
training_accuracy_list = []
test_accuracy_list = []
for k in range(5, 18, 2):
    training_accuracy = []
    test_accuracy = []
    for i in range(1, 200, 1):
        X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.20, random_state=i)
        knn = KNeighborsClassifier(n_neighbors=k)
        knn.fit(X_train, Y_train)
        y_pred_train = knn.predict(X_train)
        y_pred_test = knn.predict(X_test)
        training_accuracy.append(accuracy_score(Y_train, y_pred_train))
        test_accuracy.append(accuracy_score(Y_test, y_pred_test))
    training_accuracy_list.append(np.mean(training_accuracy).round(2))
    test_accuracy_list.append(np.mean(test_accuracy).round(2))
print("Training Accuracies:", training_accuracy_list)
print("Test Accuracies:", test_accuracy_list)

import matplotlib.pyplot as plt
plt.scatter(range(5, 18, 2),training_accuracy_list,color='blue')
plt.plot(range(5, 18, 2),training_accuracy_list,color='black')
plt.scatter(range(5, 18, 2),test_accuracy_list,color='red')
plt.plot(range(5, 18, 2),test_accuracy_list,color='black')
plt.show()
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.linear_model import LogisticRegression
Logreg = LogisticRegression()
Logreg.fit(SS_X,Y)
y_pred = Logreg.predict(X)
y_pred

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

accuracy = accuracy_score(Y_test, Y_test_pred)
precision = precision_score(Y_test, Y_test_pred, average='weighted')
recall = recall_score(Y_test, Y_test_pred, average='weighted')
f1 = f1_score(Y_test, Y_test_pred, average='weighted')
print("Accuracy:", accuracy.round(3))
print("Precision:", precision.round(3))
print("Recall:", recall.round(3))
print("F1-Score:", f1.round(3))

import numpy as np
import matplotlib.pyplot as plt
from matplotlib.colors import ListedColormap

cmap_light = ListedColormap(["red","blue","green"])
cmap_bold = ListedColormap(["red","green","blue"])

for i, j in enumerate(np.unique(Y)):
  plt.scatter(X_test[Y_test == j, 0], X_test[Y_test == j, 1],
              c=cmap_light(i), label=j)

plt.scatter(X_test[Y_test != j, 0], X_test[Y_test != j, 1],
            c='gray', label='Other')

plt.xlim(X_test[:, 0].min() - 1, X_test[:, 0].max() + 1)
plt.ylim(X_test[:, 1].min() - 1, X_test[:, 1].max() + 1)
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.title('Decision Boundaries for KNN (k=)')
plt.legend()
plt.show()

import numpy as np
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.neighbors import KNeighborsClassifier

pca = PCA(n_components=2)
SS_X_pca = pca.fit_transform(SS_X)

knn = KNeighborsClassifier(n_neighbors=17)
knn.fit(SS_X_pca, Y)

x_min, x_max = SS_X_pca[:, 0].min() - 1, SS_X_pca[:, 0].max() + 1
y_min, y_max = SS_X_pca[:, 1].min() - 1, SS_X_pca[:, 1].max() + 1
xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),
                     np.arange(y_min, y_max, 0.1))
Z = knn.predict(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)

plt.contourf(xx, yy, Z, alpha=0.4)
plt.scatter(SS_X_pca[:, 0], SS_X_pca[:, 1], c=Y, s=20, edgecolor='k')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.title('Decision Boundaries with PCA (2 components)')
plt.show()


















































