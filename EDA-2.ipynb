import pandas as pd
df = pd.read_csv('/Users/junnu/Downloads/Data Science /Assignment Questions/EDA2/adult_with_headers.csv')
df
print(df.describe())
print(df.dtypes)
numerical_data = df.select_dtypes(include=['int64', 'float64'])
categorical_data = df.select_dtypes(include=['object'])
print ('numarical data :',numerical_data)
print ('chatagorical data :',categorical_data)

missing_values = df.isnull().sum()
numerical_columns = df.select_dtypes(include=['int64', 'float64']).columns
df[numerical_columns] = df[numerical_columns].fillna(df[numerical_columns].mean())
categorical_columns = df.select_dtypes(include=['object']).columns
df[categorical_columns] = df[categorical_columns].fillna(df[categorical_columns].mode().iloc[0])
print(df.isnull().sum())

import pandas as pd
from sklearn.preprocessing import StandardScaler, MinMaxScaler
scaler = StandardScaler()
scaled_data = scaler.fit_transform(df[numerical_columns])
df_scaled = pd.DataFrame(scaled_data, columns=numerical_columns)

scaler = MinMaxScaler()
scaled_data = scaler.fit_transform(df[numerical_columns])
df_minmax = pd.DataFrame(scaled_data, columns=numerical_columns)
print('scaling techniques applyed sucessfully')

import pandas as pd
for col in categorical_columns:
    if df[col].nunique() < 5:
        df_dummies = pd.get_dummies(df[col], prefix=col)
        df = pd.concat([df, df_dummies], axis=1)
        df.drop(col, axis=1, inplace=True)
print(df.head())

from sklearn.preprocessing import LabelEncoder
categorical_columns = df.select_dtypes(include=['object']).columns
for col in categorical_columns:
    if df[col].nunique() > 5:
        le = LabelEncoder()
        df[col] = le.fit_transform(df[col])
print(df.head())


import pandas as pd
correlation_matrix = numerical_data.corr()
print(correlation_matrix)

import numpy as np
skewed_feature = df['fnlwgt']
df['fnlwgt'] = np.log(df['fnlwgt'])
print("Log transformation was chosen for the 'Flight_weight' feature because it is skewed to the right.")
print("Log transformation compresses the range of the data and reduces the influence of outliers.")



from sklearn.ensemble import IsolationForest
iso = IsolationForest(contamination='auto')
iso.fit(numerical_data)
outlier_preds = iso.predict(numerical_data)
outlier_index = outlier_preds == -1
df_clean = numerical_data[~outlier_index]
print(f"Number of outlier samples: {sum(outlier_index)}")


import ppscore as pps
pps_matrix = df.corr()
print(pps_matrix)

corr_matrix = df.corr()
print(corr_matrix)

print("Based on the PPS matrix, the following features have the strongest relationships:")
for i in range(len(pps_matrix.columns)):
    for j in range(i+1, len(pps_matrix.columns)):
        if abs(pps_matrix.iloc[i, j]) > 0.7:
            print(f"* {pps_matrix.columns[i]} and {pps_matrix.columns[j]}")




































